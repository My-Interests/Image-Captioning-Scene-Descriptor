# Motivation for Image Captioning

To generate a gramatically correct sentence which can accurately describe the scene of an image, enabling any individual to visualize the image mentally. Instead of simply detecting objects, the network aims to establish a relationship among entities in the image.

# Image Captioning using Uni-Directional and Bi-Directional LSTM

Image Features are extracted using InceptionV3 model(Pretrained) 
Captioning Model is trained on the Flickr8k Dataset

Execute 'run.py' in the 'Flickr' directory

The parent folder of this repository should contain the trained caption_model weights.

The Image Captioning Model is deployed as a REST API i.e., the web-app as well as our Flutter Application makes API calls to the server by sending an image and the server responds with a caption

The Web-App displays the Bidirectional as well as Uni directional Approaches side-by-side and also a table for the accuracy of each predicted word

# Test Results:
![Football players](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture.JPG)
![Snowy Scene](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture2.JPG)
![Surfboarding](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture4.JPG)
![Jumping Dog](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture7.JPG)

The BLEU Metric has been used to evaluate the test images. A higher BLEU rating (closer to 1) corresponds to an accurate description

The captions are generated by the model on a Laptop CPU which results in a higher processing time
Deploying the model on the cloud can improve performance
